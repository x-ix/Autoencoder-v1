# -*- coding: utf-8 -*-
"""AE_Learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FeYQC_20awdXHgUVwiPd9c2bX09w99-b
"""

#Loading data

# Used for machine learning operations
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
from io import BytesIO
import numpy as np
from sklearn.neighbors import NearestNeighbors

# Used for dataloading/handling data
import os
import requests
from torch.utils.data import Dataset
import torchvision.transforms as T
from torch.utils.data import DataLoader

# Other imports
from tqdm import tqdm  # tqdm is used for creating progress bars during training loops


import torch
from torch.utils.data import Dataset
from PIL import Image
import requests
from io import BytesIO
import os

class URLDataset(Dataset):

    def __init__(self, url_df=None, root_dir=None, transform=None):
        self.url_df = url_df
        self.root_dir = root_dir
        self.transform = transform
        self.use_urls = url_df is not None

        if not self.use_urls:
            self.image_list = os.listdir(root_dir)

    def __len__(self):
        return len(self.url_df) if self.use_urls else len(self.image_list)

    def __getitem__(self, idx):
        if self.use_urls:
            url = self.url_df.iloc[idx, 0]  # Assuming the URL is in the first column of url_df

            try:
                response = requests.get(url)
                image = Image.open(BytesIO(response.content)).convert('RGB')
            except Exception as e:
                print(f"Error loading image from URL: {url}")
                print(e)
                return None, -1  # Return a placeholder value for the label
        else:
            image_path = os.path.join(self.root_dir, self.image_list[idx])
            image = Image.open(image_path).convert('RGB')

        if self.transform:
            image = self.transform(image)

        return image, -1  # Return a placeholder value for the label

import pandas as pd
from torch.utils.data import DataLoader
from torchvision import transforms

# Assuming your DataFrame (url_df) has a column named 'urls' containing the image URLs
url_df = pd.read_csv('image_data_urls.csv')

transforms = transforms.Compose([transforms.ToTensor()])

# Create dataset with URLs
url_dataset = URLDataset(url_df=url_df, transform=transforms)

# ... Rest of the code for loading folder data (if needed)
# Assuming you already have 'full_dataset' and 'train_loader' and 'val_loader' defined.

# Create DataLoader for the URL dataset
url_dataloader = DataLoader(url_dataset, batch_size=32, shuffle=True)

# Use 'full_dataset', 'train_loader', 'val_loader', and 'url_dataloader' as needed in the rest of your code.

def display_sample_images(url_loader, num_images=5):
    """
    Fetch and display a few rows of images from the URLDataset using DataLoader.
    Args:
    url_loader: DataLoader for URLDataset.
    num_images (optional): Number of sample images to display.
    """
    import matplotlib.pyplot as plt

    url_loader_iter = iter(url_loader)
    fig, axes = plt.subplots(num_images, 1, figsize=(6, 6*num_images))
    for i in range(num_images):
        images, _ = next(url_loader_iter)
        # Convert the image tensor to numpy and transpose it to (H, W, C) format for display.
        image = images[0].permute(1, 2, 0).numpy()
        axes[i].imshow(image)
        axes[i].axis('off')

    plt.tight_layout()
    plt.show()

# Assuming you have already created the DataLoader 'dataloader' from the second code snippet
#display_sample_images(dataloader, num_images=5)

#Undercomplete autoencoder

class ConvEncoder(nn.Module):
    """
    A simple Convolutional Encoder Model
    """

    def __init__(self):
        super().__init__()

        self.conv1 = nn.Conv2d(3, 16, (3, 3), padding=(1, 1))
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool1 = nn.MaxPool2d((2, 2))

        self.conv2 = nn.Conv2d(16, 32, (3, 3), padding=(1, 1))
        self.relu2 = nn.ReLU(inplace=True)
        self.maxpool2 = nn.MaxPool2d((2, 2))

        self.conv3 = nn.Conv2d(32, 64, (3, 3), padding=(1, 1))
        self.relu3 = nn.ReLU(inplace=True)
        self.maxpool3 = nn.MaxPool2d((2, 2))

        self.conv4 = nn.Conv2d(64, 128, (3, 3), padding=(1, 1))
        self.relu4 = nn.ReLU(inplace=True)
        self.maxpool4 = nn.MaxPool2d((2, 2))

        self.conv5 = nn.Conv2d(128, 256, (3, 3), padding=(1, 1))
        self.relu5 = nn.ReLU(inplace=True)
        self.maxpool5 = nn.MaxPool2d((2, 2))

    def forward(self, x):
        # Downscale the image with conv maxpool etc.
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)

        x = self.conv2(x)
        x = self.relu2(x)
        x = self.maxpool2(x)

        x = self.conv3(x)
        x = self.relu3(x)
        x = self.maxpool3(x)

        x = self.conv4(x)
        x = self.relu4(x)
        x = self.maxpool4(x)

        x = self.conv5(x)
        x = self.relu5(x)
        x = self.maxpool5(x)

        return x

class ConvDecoder(nn.Module):
    """
    A simple Convolutional Decoder Model
    """

    def __init__(self):
        super().__init__()
        self.deconv1 = nn.ConvTranspose2d(256, 128, (2, 2), stride=(2, 2))
        self.relu1 = nn.ReLU(inplace=True)

        self.deconv2 = nn.ConvTranspose2d(128, 64, (2, 2), stride=(2, 2))
        self.relu2 = nn.ReLU(inplace=True)

        self.deconv3 = nn.ConvTranspose2d(64, 32, (2, 2), stride=(2, 2))
        self.relu3 = nn.ReLU(inplace=True)

        self.deconv4 = nn.ConvTranspose2d(32, 16, (2, 2), stride=(2, 2))
        self.relu4 = nn.ReLU(inplace=True)

        self.deconv5 = nn.ConvTranspose2d(16, 3, (2, 2), stride=(2, 2))
        self.relu5 = nn.ReLU(inplace=True)

    def forward(self, x):
         # Upscale the image with convtranspose etc.
        x = self.deconv1(x)
        x = self.relu1(x)

        x = self.deconv2(x)
        x = self.relu2(x)

        x = self.deconv3(x)
        x = self.relu3(x)

        x = self.deconv4(x)
        x = self.relu4(x)

        x = self.deconv5(x)
        x = self.relu5(x)
        return x

def train_step(encoder, decoder, train_loader, loss_fn, optimizer, device):
    """
    Performs a single training step
    Args:
    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder
    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder
    train_loader: PyTorch dataloader, containing (images, images).
    loss_fn: PyTorch loss_fn, computes loss between 2 images.
    optimizer: PyTorch optimizer.
    device: "cuda" or "cpu"
    Returns: Train Loss
    """
    #  Set networks to train mode.
    encoder.train()
    decoder.train()

    for batch_idx, (train_img, target_img) in enumerate(train_loader):
        # Move images to device
        train_img = train_img.to(device)
        target_img = target_img.to(device)

        # Zero grad the optimizer
        optimizer.zero_grad()
        # Feed the train images to encoder
        enc_output = encoder(train_img)
        # The output of encoder is input to decoder !
        dec_output = decoder(enc_output)

        # Decoder output is reconstructed image
        # Compute loss with it and orginal image which is target image.
        loss = loss_fn(dec_output, target_img)
        # Backpropogate
        loss.backward()
        # Apply the optimizer to network by calling step.
        optimizer.step()
    # Return the loss
    return loss.item()

def val_step(encoder, decoder, val_loader, loss_fn, device):
    """
    Performs a single training step
    Args:
    encoder: A convolutional Encoder. E.g. torch_model ConvEncoder
    decoder: A convolutional Decoder. E.g. torch_model ConvDecoder
    val_loader: PyTorch dataloader, containing (images, images).
    loss_fn: PyTorch loss_fn, computes loss between 2 images.
    device: "cuda" or "cpu"
    Returns: Validation Loss
    """

    # Set to eval mode.
    encoder.eval()
    decoder.eval()

    # We don't need to compute gradients while validating.
    with torch.no_grad():
        for batch_idx, (train_img, target_img) in enumerate(val_loader):
            # Move to device
            train_img = train_img.to(device)
            target_img = target_img.to(device)

            # Again as train. Feed encoder the train image.
            enc_output = encoder(train_img)
            # Decoder takes encoder output and reconstructs the image.
            dec_output = decoder(enc_output)

            # Validation loss for encoder and decoder.
            loss = loss_fn(dec_output, target_img)
    # Return the loss
    return loss.item()

# Simplified Training Script
import torch
import torchvision.transforms as T

transforms = T.Compose([T.ToTensor()]) # Normalize the pixels and convert to tensor.

full_dataset = URLDataset("../data/", transforms) # Create folder dataset.

train_size = 0.75
val_size = 1 - train_size

# Split data to train and test
train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])


# Create DataLoader for the URL dataset
url_dataloader = DataLoader(url_dataset, batch_size=32, shuffle=True)

# Create DataLoader for the folder dataset (assuming you have 'full_dataset' already defined)
folder_dataloader = DataLoader(full_dataset, batch_size=32, shuffle=True)

# Time to Train !!!
EPOCHS = 10
max_loss = float('inf')  # Initialize max_loss to a large value

# Usual Training Loop
for epoch in tqdm(range(EPOCHS)):
    train_loss = train_step(encoder, decoder, train_loader, loss_fn, optimizer, device=device)
    print(f"Epochs = {epoch}, Training Loss : {train_loss}")

    val_loss = val_step(encoder, decoder, val_loader, loss_fn, device=device)
    print(f"Epochs = {epoch}, Validation Loss : {val_loss}")

    # Simple Best Model saving
    if val_loss < max_loss:
        print("Validation Loss decreased, saving new best model")
        torch.save(encoder.state_dict(), "encoder_model.pt")
        torch.save(decoder.state_dict(), "decoder_model.pt")
        max_loss = val_loss  # Update max_loss with the new best validation loss

# Save the feature representations.
EMBEDDING_SHAPE = (1, 256, 16, 16) # This we know from our encoder

# We need feature representations for complete dataset not just train and validation.
# Hence we use full loader here.
embedding = create_embedding(encoder, full_loader, EMBEDDING_SHAPE, device)

# Convert embedding to numpy and save them
numpy_embedding = embedding.cpu().detach().numpy()
num_images = numpy_embedding.shape[0]

# Save the embeddings for complete dataset, not just train
flattened_embedding = numpy_embedding.reshape((num_images, -1))
np.save("data_embedding.npy", flattened_embedding)